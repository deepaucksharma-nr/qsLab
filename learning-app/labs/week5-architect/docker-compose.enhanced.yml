version: '3.8'

services:
  # KRaft Controller Nodes (replacing Zookeeper)
  controller-1:
    image: confluentinc/cp-kafka:7.5.0
    container_name: week5-controller-1
    hostname: controller-1
    restart: unless-stopped
    ports:
      - "19093:9093"
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: 'controller'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@controller-1:9093,2@controller-2:9093,3@controller-3:9093'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT'
      KAFKA_LISTENERS: 'CONTROLLER://controller-1:9093'
      KAFKA_LOG_DIRS: '/var/lib/kafka/data'
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS: 300000
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: localhost
      KAFKA_HEAP_OPTS: "-Xmx1G -Xms1G"
      CLUSTER_ID: 'q1XqJg-TS8SbhNezWD1bEg'
    volumes:
      - controller-1-data:/var/lib/kafka/data
      - ./kafka/update_run.sh:/tmp/update_run.sh
    command: "bash -c '/tmp/update_run.sh && /etc/confluent/docker/run'"
    networks:
      - week5-network
    healthcheck:
      test: ["CMD-SHELL", "kafka-metadata shell --snapshot /var/lib/kafka/data/__cluster_metadata-0/00000000000000000000.log --print-brokers || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

  controller-2:
    image: confluentinc/cp-kafka:7.5.0
    container_name: week5-controller-2
    hostname: controller-2
    restart: unless-stopped
    ports:
      - "29093:9093"
    environment:
      KAFKA_NODE_ID: 2
      KAFKA_PROCESS_ROLES: 'controller'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@controller-1:9093,2@controller-2:9093,3@controller-3:9093'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT'
      KAFKA_LISTENERS: 'CONTROLLER://controller-2:9093'
      KAFKA_LOG_DIRS: '/var/lib/kafka/data'
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS: 300000
      KAFKA_JMX_PORT: 9102
      KAFKA_JMX_HOSTNAME: localhost
      KAFKA_HEAP_OPTS: "-Xmx1G -Xms1G"
      CLUSTER_ID: 'q1XqJg-TS8SbhNezWD1bEg'
    volumes:
      - controller-2-data:/var/lib/kafka/data
      - ./kafka/update_run.sh:/tmp/update_run.sh
    command: "bash -c '/tmp/update_run.sh && /etc/confluent/docker/run'"
    networks:
      - week5-network
    healthcheck:
      test: ["CMD-SHELL", "kafka-metadata shell --snapshot /var/lib/kafka/data/__cluster_metadata-0/00000000000000000000.log --print-brokers || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

  controller-3:
    image: confluentinc/cp-kafka:7.5.0
    container_name: week5-controller-3
    hostname: controller-3
    restart: unless-stopped
    ports:
      - "39093:9093"
    environment:
      KAFKA_NODE_ID: 3
      KAFKA_PROCESS_ROLES: 'controller'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@controller-1:9093,2@controller-2:9093,3@controller-3:9093'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT'
      KAFKA_LISTENERS: 'CONTROLLER://controller-3:9093'
      KAFKA_LOG_DIRS: '/var/lib/kafka/data'
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS: 300000
      KAFKA_JMX_PORT: 9103
      KAFKA_JMX_HOSTNAME: localhost
      KAFKA_HEAP_OPTS: "-Xmx1G -Xms1G"
      CLUSTER_ID: 'q1XqJg-TS8SbhNezWD1bEg'
    volumes:
      - controller-3-data:/var/lib/kafka/data
      - ./kafka/update_run.sh:/tmp/update_run.sh
    command: "bash -c '/tmp/update_run.sh && /etc/confluent/docker/run'"
    networks:
      - week5-network
    healthcheck:
      test: ["CMD-SHELL", "kafka-metadata shell --snapshot /var/lib/kafka/data/__cluster_metadata-0/00000000000000000000.log --print-brokers || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

  # Kafka Broker Nodes
  broker-1:
    image: confluentinc/cp-kafka:7.5.0
    container_name: week5-broker-1
    hostname: broker-1
    restart: unless-stopped
    depends_on:
      controller-1:
        condition: service_healthy
      controller-2:
        condition: service_healthy
      controller-3:
        condition: service_healthy
    ports:
      - "19092:19092"
      - "9092:9092"
      - "9104:9104"
    environment:
      KAFKA_NODE_ID: 4
      KAFKA_PROCESS_ROLES: 'broker'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@controller-1:9093,2@controller-2:9093,3@controller-3:9093'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://broker-1:9092,PLAINTEXT_HOST://localhost:19092'
      KAFKA_LISTENERS: 'PLAINTEXT://broker-1:9092,PLAINTEXT_HOST://0.0.0.0:19092'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'
      KAFKA_LOG_DIRS: '/var/lib/kafka/data'
      
      # Performance & Production Settings
      KAFKA_NUM_NETWORK_THREADS: 16
      KAFKA_NUM_IO_THREADS: 16
      KAFKA_SOCKET_SEND_BUFFER_BYTES: 1048576
      KAFKA_SOCKET_RECEIVE_BUFFER_BYTES: 1048576
      KAFKA_SOCKET_REQUEST_MAX_BYTES: 104857600
      KAFKA_NUM_REPLICA_FETCHERS: 4
      KAFKA_REPLICA_FETCH_MAX_BYTES: 1048576
      
      # Replication Settings
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      
      # Production Configuration
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'
      KAFKA_DELETE_TOPIC_ENABLE: 'true'
      KAFKA_COMPRESSION_TYPE: 'lz4'
      KAFKA_LOG_RETENTION_HOURS: 168  # 7 days
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS: 300000
      KAFKA_LOG_CLEANUP_POLICY: "delete"
      
      # JMX Configuration
      KAFKA_JMX_PORT: 9104
      KAFKA_JMX_HOSTNAME: localhost
      
      # JVM Settings
      KAFKA_HEAP_OPTS: "-Xmx3G -Xms3G"
      KAFKA_JVM_PERFORMANCE_OPTS: >-
        -XX:+UseG1GC
        -XX:MaxGCPauseMillis=20
        -XX:InitiatingHeapOccupancyPercent=35
        -XX:+ExplicitGCInvokesConcurrent
        -XX:MaxInlineLevel=15
        -Djava.awt.headless=true
      
      CLUSTER_ID: 'q1XqJg-TS8SbhNezWD1bEg'
    volumes:
      - broker-1-data:/var/lib/kafka/data
      - ./kafka/update_run.sh:/tmp/update_run.sh
    command: "bash -c '/tmp/update_run.sh && /etc/confluent/docker/run'"
    networks:
      - week5-network
    healthcheck:
      test: ["CMD-SHELL", "kafka-broker-api-versions --bootstrap-server localhost:9092"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "5"

  broker-2:
    image: confluentinc/cp-kafka:7.5.0
    container_name: week5-broker-2
    hostname: broker-2
    restart: unless-stopped
    depends_on:
      controller-1:
        condition: service_healthy
      controller-2:
        condition: service_healthy
      controller-3:
        condition: service_healthy
    ports:
      - "29092:29092"
      - "9093:9093"
      - "9105:9105"
    environment:
      KAFKA_NODE_ID: 5
      KAFKA_PROCESS_ROLES: 'broker'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@controller-1:9093,2@controller-2:9093,3@controller-3:9093'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://broker-2:9093,PLAINTEXT_HOST://localhost:29092'
      KAFKA_LISTENERS: 'PLAINTEXT://broker-2:9093,PLAINTEXT_HOST://0.0.0.0:29092'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'
      KAFKA_LOG_DIRS: '/var/lib/kafka/data'
      
      # Performance & Production Settings (same as broker-1)
      KAFKA_NUM_NETWORK_THREADS: 16
      KAFKA_NUM_IO_THREADS: 16
      KAFKA_SOCKET_SEND_BUFFER_BYTES: 1048576
      KAFKA_SOCKET_RECEIVE_BUFFER_BYTES: 1048576
      KAFKA_SOCKET_REQUEST_MAX_BYTES: 104857600
      KAFKA_NUM_REPLICA_FETCHERS: 4
      KAFKA_REPLICA_FETCH_MAX_BYTES: 1048576
      
      # Replication Settings
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      
      # Production Configuration
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'
      KAFKA_DELETE_TOPIC_ENABLE: 'true'
      KAFKA_COMPRESSION_TYPE: 'lz4'
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS: 300000
      KAFKA_LOG_CLEANUP_POLICY: "delete"
      
      # JMX Configuration
      KAFKA_JMX_PORT: 9105
      KAFKA_JMX_HOSTNAME: localhost
      
      # JVM Settings
      KAFKA_HEAP_OPTS: "-Xmx3G -Xms3G"
      KAFKA_JVM_PERFORMANCE_OPTS: >-
        -XX:+UseG1GC
        -XX:MaxGCPauseMillis=20
        -XX:InitiatingHeapOccupancyPercent=35
        -XX:+ExplicitGCInvokesConcurrent
        -XX:MaxInlineLevel=15
        -Djava.awt.headless=true
      
      CLUSTER_ID: 'q1XqJg-TS8SbhNezWD1bEg'
    volumes:
      - broker-2-data:/var/lib/kafka/data
      - ./kafka/update_run.sh:/tmp/update_run.sh
    command: "bash -c '/tmp/update_run.sh && /etc/confluent/docker/run'"
    networks:
      - week5-network
    healthcheck:
      test: ["CMD-SHELL", "kafka-broker-api-versions --bootstrap-server localhost:9093"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "5"

  broker-3:
    image: confluentinc/cp-kafka:7.5.0
    container_name: week5-broker-3
    hostname: broker-3
    restart: unless-stopped
    depends_on:
      controller-1:
        condition: service_healthy
      controller-2:
        condition: service_healthy
      controller-3:
        condition: service_healthy
    ports:
      - "39092:39092"
      - "9094:9094"
      - "9106:9106"
    environment:
      KAFKA_NODE_ID: 6
      KAFKA_PROCESS_ROLES: 'broker'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@controller-1:9093,2@controller-2:9093,3@controller-3:9093'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://broker-3:9094,PLAINTEXT_HOST://localhost:39092'
      KAFKA_LISTENERS: 'PLAINTEXT://broker-3:9094,PLAINTEXT_HOST://0.0.0.0:39092'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'
      KAFKA_LOG_DIRS: '/var/lib/kafka/data'
      
      # Performance & Production Settings (same as broker-1)
      KAFKA_NUM_NETWORK_THREADS: 16
      KAFKA_NUM_IO_THREADS: 16
      KAFKA_SOCKET_SEND_BUFFER_BYTES: 1048576
      KAFKA_SOCKET_RECEIVE_BUFFER_BYTES: 1048576
      KAFKA_SOCKET_REQUEST_MAX_BYTES: 104857600
      KAFKA_NUM_REPLICA_FETCHERS: 4
      KAFKA_REPLICA_FETCH_MAX_BYTES: 1048576
      
      # Replication Settings
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      
      # Production Configuration
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'
      KAFKA_DELETE_TOPIC_ENABLE: 'true'
      KAFKA_COMPRESSION_TYPE: 'lz4'
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS: 300000
      KAFKA_LOG_CLEANUP_POLICY: "delete"
      
      # JMX Configuration
      KAFKA_JMX_PORT: 9106
      KAFKA_JMX_HOSTNAME: localhost
      
      # JVM Settings
      KAFKA_HEAP_OPTS: "-Xmx3G -Xms3G"
      KAFKA_JVM_PERFORMANCE_OPTS: >-
        -XX:+UseG1GC
        -XX:MaxGCPauseMillis=20
        -XX:InitiatingHeapOccupancyPercent=35
        -XX:+ExplicitGCInvokesConcurrent
        -XX:MaxInlineLevel=15
        -Djava.awt.headless=true
      
      CLUSTER_ID: 'q1XqJg-TS8SbhNezWD1bEg'
    volumes:
      - broker-3-data:/var/lib/kafka/data
      - ./kafka/update_run.sh:/tmp/update_run.sh
    command: "bash -c '/tmp/update_run.sh && /etc/confluent/docker/run'"
    networks:
      - week5-network
    healthcheck:
      test: ["CMD-SHELL", "kafka-broker-api-versions --bootstrap-server localhost:9094"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "5"

  # Schema Registry Cluster
  schema-registry-1:
    image: confluentinc/cp-schema-registry:7.5.0
    container_name: week5-schema-registry-1
    hostname: schema-registry-1
    restart: unless-stopped
    depends_on:
      broker-1:
        condition: service_healthy
      broker-2:
        condition: service_healthy
      broker-3:
        condition: service_healthy
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry-1
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'broker-1:9092,broker-2:9093,broker-3:9094'
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
      SCHEMA_REGISTRY_KAFKASTORE_TOPIC: _schemas
      SCHEMA_REGISTRY_KAFKASTORE_TOPIC_REPLICATION_FACTOR: 3
      SCHEMA_REGISTRY_LEADER_ELIGIBILITY: "true"
      SCHEMA_REGISTRY_MODE_MUTABILITY: "true"
      SCHEMA_REGISTRY_LOG4J_ROOT_LOGLEVEL: INFO
    volumes:
      - schema-registry-1-data:/var/lib/schema-registry
    networks:
      - week5-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/subjects"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

  # Kafka UI
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: week5-kafka-ui
    restart: unless-stopped
    depends_on:
      broker-1:
        condition: service_healthy
      broker-2:
        condition: service_healthy
      broker-3:
        condition: service_healthy
      schema-registry-1:
        condition: service_healthy
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: week5-architect-cluster
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: broker-1:9092,broker-2:9093,broker-3:9094
      KAFKA_CLUSTERS_0_SCHEMAREGISTRY: http://schema-registry-1:8081
      KAFKA_CLUSTERS_0_JMXPORT: 9104,9105,9106
      KAFKA_CLUSTERS_0_READONLY: "false"
      DYNAMIC_CONFIG_ENABLED: "true"
      LOGGING_LEVEL_ROOT: INFO
    networks:
      - week5-network
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:8080/actuator/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

  # Prometheus for Metrics Collection
  prometheus:
    image: prom/prometheus:latest
    container_name: week5-prometheus
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus-prod.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--storage.tsdb.retention.size=10GB'
      - '--web.enable-admin-api'
      - '--web.enable-lifecycle'
    networks:
      - week5-network
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

  # Grafana for Dashboards
  grafana:
    image: grafana/grafana:latest
    container_name: week5-grafana
    restart: unless-stopped
    depends_on:
      prometheus:
        condition: service_healthy
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin
      GF_USERS_ALLOW_SIGN_UP: "false"
      GF_INSTALL_PLUGINS: grafana-piechart-panel
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
    networks:
      - week5-network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

  # New Relic Infrastructure Agent
  newrelic-infra:
    image: newrelic/infrastructure:latest
    container_name: week5-newrelic-infra
    restart: unless-stopped
    depends_on:
      broker-1:
        condition: service_healthy
      broker-2:
        condition: service_healthy
      broker-3:
        condition: service_healthy
    cap_add:
      - SYS_PTRACE
    network_mode: host
    pid: host
    privileged: true
    environment:
      NRIA_LICENSE_KEY: ${NEW_RELIC_LICENSE_KEY}
      NRIA_DISPLAY_NAME: "Week5-Architect-Lab"
      NRIA_CUSTOM_ATTRIBUTES: '{"cluster":"week5-architect","purpose":"production-architecture","week":"5","lab_type":"enterprise","kafka_version":"kraft"}'
      NRIA_LOG_LEVEL: "info"
      NRIA_ENABLE_PROCESS_METRICS: true
      NRIA_METRICS_PROCESS_SAMPLE_RATE: 30
      NRIA_PAYLOAD_COMPRESSION_LEVEL: 6
    volumes:
      - /:/host:ro
      - /var/run/docker.sock:/var/run/docker.sock
      - ./configs:/etc/newrelic-infra/integrations.d:ro
      - newrelic-data:/var/db/newrelic-infra
    healthcheck:
      test: ["CMD", "newrelic-infra-ctl", "status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

  # HAProxy Load Balancer
  haproxy:
    image: haproxy:2.8-alpine
    container_name: week5-haproxy
    restart: unless-stopped
    depends_on:
      broker-1:
        condition: service_healthy
      broker-2:
        condition: service_healthy
      broker-3:
        condition: service_healthy
    ports:
      - "9092:9092"  # Unified Kafka entry point
      - "8404:8404"  # HAProxy stats
    volumes:
      - ./haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro
    networks:
      - week5-network
    healthcheck:
      test: ["CMD", "haproxy", "-c", "-f", "/usr/local/etc/haproxy/haproxy.cfg"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "30m"
        max-file: "3"

volumes:
  controller-1-data:
    driver: local
  controller-2-data:
    driver: local
  controller-3-data:
    driver: local
  broker-1-data:
    driver: local
  broker-2-data:
    driver: local
  broker-3-data:
    driver: local
  schema-registry-1-data:
    driver: local
  prometheus-data:
    driver: local
  grafana-data:
    driver: local
  newrelic-data:
    driver: local

networks:
  week5-network:
    name: week5-architect-network
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.24.0.0/16