version: '3.8'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: week2-zookeeper
    hostname: zookeeper
    restart: unless-stopped
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
      ZOOKEEPER_MAX_CLIENT_CNXNS: 60
      ZOOKEEPER_AUTOPURGE_SNAP_RETAIN_COUNT: 3
      ZOOKEEPER_AUTOPURGE_PURGE_INTERVAL: 24
      ZOOKEEPER_LOG4J_ROOT_LOGLEVEL: INFO
    ports:
      - "22181:2181"
    volumes:
      - week2-zookeeper-data:/var/lib/zookeeper/data
      - week2-zookeeper-logs:/var/lib/zookeeper/log
    networks:
      - week2-network
    healthcheck:
      test: ["CMD", "echo", "ruok", "|", "nc", "localhost", "2181", "|", "grep", "imok"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 40s
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

  broker:
    image: confluentinc/cp-kafka:7.5.0
    container_name: week2-broker
    hostname: broker
    restart: unless-stopped
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "29092:29092"
      - "9092:9092"
      - "9101:9101"
    environment:
      # Basic Configuration
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      
      # Replication Configuration
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_MIN_INSYNC_REPLICAS: 1
      
      # Performance Configuration
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_NUM_NETWORK_THREADS: 8
      KAFKA_NUM_IO_THREADS: 8
      KAFKA_SOCKET_SEND_BUFFER_BYTES: 102400
      KAFKA_SOCKET_RECEIVE_BUFFER_BYTES: 102400
      
      # JMX Configuration
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: localhost
      KAFKA_JMX_OPTS: "-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false"
      
      # Log Configuration
      KAFKA_LOG_DIRS: /var/lib/kafka/data
      KAFKA_DELETE_TOPIC_ENABLE: 'true'
      KAFKA_LOG_SEGMENT_BYTES: 1048576  # 1MB for faster tombstone testing
      KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS: 30000
      KAFKA_LOG_RETENTION_HOURS: 1
      KAFKA_LOG_CLEANUP_POLICY: "delete"
      KAFKA_COMPRESSION_TYPE: "snappy"
      
      # Tombstone Configuration
      KAFKA_LOG_CLEANER_ENABLE: 'true'
      KAFKA_LOG_CLEANER_DELETE_RETENTION_MS: 86400000  # 1 day
      KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS: 0
      
      # Additional Settings
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_AUTO_LEADER_REBALANCE_ENABLE: 'true'
      KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS: 300
    volumes:
      - week2-broker-data:/var/lib/kafka/data
      - week2-broker-logs:/var/lib/kafka/logs
    networks:
      - week2-network
    healthcheck:
      test: ["CMD-SHELL", "kafka-broker-api-versions --bootstrap-server localhost:9092"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "5"

  schema-registry:
    image: confluentinc/cp-schema-registry:7.5.0
    container_name: week2-schema-registry
    hostname: schema-registry
    restart: unless-stopped
    depends_on:
      broker:
        condition: service_healthy
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'broker:9092'
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
      SCHEMA_REGISTRY_KAFKASTORE_TOPIC: _schemas
      SCHEMA_REGISTRY_DEBUG: 'true'
      SCHEMA_REGISTRY_KAFKASTORE_TOPIC_REPLICATION_FACTOR: 1
      SCHEMA_REGISTRY_SCHEMA_REGISTRY_INTER_INSTANCE_PROTOCOL: "http"
      SCHEMA_REGISTRY_LOG4J_ROOT_LOGLEVEL: INFO
      SCHEMA_REGISTRY_KAFKASTORE_INIT_TIMEOUT_MS: 60000
    volumes:
      - schema-registry-data:/var/lib/schema-registry
    networks:
      - week2-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/subjects"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: week2-kafka-ui
    restart: unless-stopped
    depends_on:
      broker:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: week2-cluster
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: broker:9092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
      KAFKA_CLUSTERS_0_SCHEMAREGISTRY: http://schema-registry:8081
      KAFKA_CLUSTERS_0_JMXPORT: 9101
      KAFKA_CLUSTERS_0_READONLY: "false"
      DYNAMIC_CONFIG_ENABLED: "true"
      LOGGING_LEVEL_ROOT: INFO
      LOGGING_LEVEL_COM_PROVECTUS: DEBUG
    networks:
      - week2-network
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:8080/actuator/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

  # New Relic Infrastructure Agent with Kafka integration
  newrelic-infra:
    image: newrelic/infrastructure:latest
    container_name: week2-newrelic-infra
    restart: unless-stopped
    depends_on:
      broker:
        condition: service_healthy
    cap_add:
      - SYS_PTRACE
    network_mode: host
    pid: host
    privileged: true
    environment:
      NRIA_LICENSE_KEY: ${NEW_RELIC_LICENSE_KEY}
      NRIA_DISPLAY_NAME: "Week2-Builder-Lab"
      NRIA_CUSTOM_ATTRIBUTES: '{"cluster":"week2-builder","purpose":"learning-lab","week":"2","lab_type":"integration-builder"}'
      NRIA_LOG_LEVEL: "info"
      NRIA_VERBOSE: 0
      NRIA_PAYLOAD_COMPRESSION_LEVEL: 6
      NRIA_ENABLE_PROCESS_METRICS: true
      NRIA_STATUS_BACKEND_URL: "https://infrastructure-status.newrelic.com"
    volumes:
      - /:/host:ro
      - /var/run/docker.sock:/var/run/docker.sock
      - ./configs:/etc/newrelic-infra/integrations.d:ro
      - ./custom-integrations:/var/db/newrelic-infra/custom-integrations
      - newrelic-data:/var/db/newrelic-infra
    healthcheck:
      test: ["CMD", "newrelic-infra-ctl", "status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

  # Tombstone data generator for exercises
  tombstone-generator:
    image: confluentinc/cp-kafka:7.5.0
    container_name: week2-tombstone-generator
    restart: "no"
    depends_on:
      broker:
        condition: service_healthy
    environment:
      KAFKA_HEAP_OPTS: "-Xmx256M -Xms256M"
    command: |
      bash -c "
        echo 'Waiting for Kafka to be ready...'
        cub kafka-ready -b broker:9092 1 60
        
        # Create topics for tombstone exercises
        kafka-topics --create --if-not-exists --bootstrap-server broker:9092 --partitions 3 --replication-factor 1 --topic tombstone-test --config cleanup.policy=compact --config segment.ms=60000
        kafka-topics --create --if-not-exists --bootstrap-server broker:9092 --partitions 3 --replication-factor 1 --topic user-events --config cleanup.policy=compact --config min.cleanable.dirty.ratio=0.01
        kafka-topics --create --if-not-exists --bootstrap-server broker:9092 --partitions 3 --replication-factor 1 --topic inventory-updates --config cleanup.policy=compact,delete --config retention.ms=3600000
        
        # Create topics for Flex integration exercises
        kafka-topics --create --if-not-exists --bootstrap-server broker:9092 --partitions 5 --replication-factor 1 --topic flex-metrics
        kafka-topics --create --if-not-exists --bootstrap-server broker:9092 --partitions 2 --replication-factor 1 --topic flex-events
        
        echo 'Topics created successfully.'
      "
    networks:
      - week2-network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "2"

  # Development environment for building custom integrations
  dev-environment:
    image: golang:1.21-alpine
    container_name: week2-dev-env
    restart: unless-stopped
    working_dir: /workspace
    environment:
      GO111MODULE: "on"
      GOPROXY: "https://proxy.golang.org,direct"
      CGO_ENABLED: "0"
    volumes:
      - ./custom-integrations:/workspace
      - go-cache:/go/pkg/mod
      - go-build-cache:/root/.cache/go-build
    command: tail -f /dev/null
    networks:
      - week2-network
    healthcheck:
      test: ["CMD", "go", "version"]
      interval: 60s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "20m"
        max-file: "2"

volumes:
  week2-broker-data:
    driver: local
  week2-broker-logs:
    driver: local
  week2-zookeeper-data:
    driver: local
  week2-zookeeper-logs:
    driver: local
  schema-registry-data:
    driver: local
  newrelic-data:
    driver: local
  go-cache:
    driver: local
  go-build-cache:
    driver: local

networks:
  week2-network:
    name: week2-builder-network
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.21.0.0/16